{"meta":{"title":"城北徐公子","subtitle":"再努力一点","description":"人生就是这样，奋斗奋斗才是未来","author":"徐北城","url":"https://Xu-Shuhao.github.io","root":"/"},"pages":[{"title":"categories","date":"2021-08-06T07:58:07.000Z","updated":"2021-08-06T08:01:52.258Z","comments":true,"path":"categories/index.html","permalink":"https://xu-shuhao.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-08-06T07:58:23.000Z","updated":"2021-08-06T08:02:31.021Z","comments":true,"path":"tags/index.html","permalink":"https://xu-shuhao.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LeetCode-Array 个人刷题经验","slug":"LeetCode-Array","date":"2021-08-28T15:27:27.000Z","updated":"2021-08-28T16:10:52.993Z","comments":true,"path":"2021/08/28/LeetCode-Array/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/28/LeetCode-Array/","excerpt":"","text":"本篇章会介绍$Leetcode$中的Array部分的刷题经验和个人思路文章结构是: 题号 + 题名 + 题目难度 [H2] 做题时间(When) 做题思路(How) [H3] 文字思路 [H5] 代码 [H5] 总结(Conclusion) [H3] ① 如果没有说明, 文中的$i, j, k$指的是循环第$1,2,3$层的变量 题号:53 最大子序和 Easy 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 eg: 123输入：nums = [-2,1,-3,4,-1,2,1,-5,4]输出：6解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 做题时间: 2021年08月28日 23:32:28 做题思路 ① 看数据量范围 1 &lt;= nums.length &lt;= 3 * 104 -105 &lt;= nums[i] &lt;= 105 这就意味着可以用暴力求解, $O(N^2)$ ② 暴力求解 思路: 列举所有可能的情况, 两个for循环即可, 内循环(j)可以从外循环(i)开始, 并非需要从$0$开始. 优点: 思路明确, 代码量少 缺点: 时间复杂度高 执行用时：480 ms, 在所有 C++ 提交中击败了6.41%的用户 内存消耗：12.8 MB, 在所有 C++ 提交中击败了48.56%的用户 ③ 双指针求解 Ⅰ 为什么想到用双指针 因为这是一个求区间的题目, 题目要求在整体数组中找到最大值, 即求最大区间的值, 求区间的题目适合用双指针, 能把时间从暴力的$O(N^2)$降低到$O(N)$, 因为暴力包含了很多没必要的计算样例. Ⅱ 双指针用两边往中间跑还是一边往另一边跑 目前没有很明确的感觉判断该用哪种, 个人是两种都试了, 先尝试的前者. 两边往中间跑 假设有两个指针, $\\hat{a}$ 在起始端, $\\hat{b}$ 在末端, 问题来了: 什么时候 $\\hat{a}$ 移动, 什么时候 $\\hat{b}$ 移动? 思考1: 较小的先动 [错误], 反例很容易想到 一边往另一边跑 $\\hat{a}$ $\\hat{b}$ 都在起始端: 什么时候 $\\hat{a}$ 移动, 什么时候 $\\hat{b}$ 移动? $\\hat{a}$ 用$i$表示, 一步步移动; $\\hat{b}$ 用$j$表示, 移动有两种情况: nums[j] &lt;=0 or 当前$i$指向点的左边及该点, 完全没有价值时 $\\hat{a}$ $\\hat{b}$ 要按照什么步伐移动? 当nums[j] &lt;=0, $j$++即可, 因为只会出现[负 正, 负 负]时, $j$要离开第一个负, 新的$sum$值才会更大 当前$i$指向点的左边及该点, 完全没有价值时:$j = i +1$ $\\hat{a}$ 和 $\\hat{b}$ 之间的值(称作$sum$) 和$max$是什么关系 12if sum &gt; max: max = sum# 关键在于放的位置 代码 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int max = -1000000; //表示历史最大值 int len = nums.size(); int j = 0; int sum = 0; //表示当前框内的和 int value_j = 0; for(int i=0; i&lt;len; i++)&#123; int value = nums[i]; int value_j = nums[j]; if(value_j&lt;=0 &amp;&amp; j&lt;i)&#123; //当j指向的是负数的时候,需要移动 j++; &#125; if(i==j)&#123; sum = value; &#125;else&#123; //其实就是i&gt;j的情况 if(sum+nums[i]&lt;=0)&#123; if(i&lt;len-1)&#123; j = i+1; &#125; // else的情况就是i已经运行到最后一个位置了, 既然不可取, 那就没必要移动j了 &#125;else&#123;//else 说明i的左端有包含的价值, 因为及时加上当前的负数, 总和仍能为正, 所以i继续往前移动即可 sum+=value; &#125; &#125; if(sum&gt;max) max = sum; &#125; return max; &#125;&#125;; ④ DP求解 正在思考中$Z^{Z^Z}$ 曾经考虑过, 设置相同size的两个Array, 分别记录当前节点的左和右的最大整数和(无果, 可能两个矩阵思路是对的, 但是表达的意思应该不是) 要不试试设置一个二维的矩阵(size*size) ? ⑤ 分治法 有空再想中$Z^{Z^Z}$ 总结​ 数组的题目很多都是和其他有关系的, 双指针呀, DP呀, 贪心呀等等, 一定要刷完之后好好总结套路, 事半功倍.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://xu-shuhao.github.io/categories/Coding/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://xu-shuhao.github.io/tags/Leetcode/"}]},{"title":"新手Pytorch笔记 (二)","slug":"新手Pytorch笔记","date":"2021-08-25T17:29:22.000Z","updated":"2021-08-26T13:14:50.977Z","comments":true,"path":"2021/08/26/新手Pytorch笔记/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/26/%E6%96%B0%E6%89%8BPytorch%E7%AC%94%E8%AE%B0/","excerpt":"","text":"基础概念 什么是torchvision, torchvision包含了什么, 有什么作用, 跟pytorch版本的对应关系是什么 torchvision是基于Pytorch学习框架， 包含： torchvision.datasets：常见的vision数据集 torchvision.models: 比较流行的模型 torchvision.transforms: 常见的图形变幻方式 torchvision.utils: 图像处理的工具包 Pytorch中的Tensor 详情请看新手PytorchTensor笔记 (一)","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/categories/Pytorch/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/tags/Pytorch/"},{"name":"Learning Note","slug":"Learning-Note","permalink":"https://xu-shuhao.github.io/tags/Learning-Note/"}]},{"title":"深入浅出图神经网络笔记","slug":"深入浅出图神经网络笔记","date":"2021-08-24T03:50:15.000Z","updated":"2021-08-26T08:17:38.199Z","comments":true,"path":"2021/08/24/深入浅出图神经网络笔记/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/","excerpt":"","text":"本篇博客只在解释作者自己在《深入浅出图神经网络》这本书的笔记，很多部分作者自己懂就不会发不出来，请谅解，如果需要这本书，请到深入浅出图神经网络自行下载。【注：仅供学习使用】 1.2 图的存储与遍历1.2.1 邻接矩阵与关联矩阵​ 在实际中，邻接矩阵往往会出现大量的0值，因此可以用稀疏矩阵的格式来存储邻接矩阵，这样可以将邻接矩阵的空间复杂度控制在$O(M)$ 的范围内。 ​ 一般定义: ​ $e &lt; n log_n (n&amp;e \\ mean \\ the \\ number\\ of\\ nodes&amp;edges)$是稀疏图 矩阵的稀疏存储方式: ​ 用邻接表存储点和点之间的关联关系. 在Tensorflow中, 用SparseTensor对象稀疏矩阵, 通过3个对象稠密矩阵indices, values, dense_shape表示稀疏矩阵. indices 数据类型: int64 二维Tensor对象 Shape: [N, ndims] values 数据类型:不限(元素值) 一维Tensor对象 Shape: [N] dense_shape 数据类型: int64 一维Tensor对象 Shape: [ndims] 1.3 图的主要四个种类 同构图(Homogeneous Graph) 节点类型和关系类型都仅有一种 异构图(Heterogenerous Graph) 节点类型or关系类型多余一种-&gt;贴合现实 属性图(Property Graph) 节点和关系(Nodes&amp;Relationship) both都有标签Label和Property -&gt;更贴合业务 非显示图(Graph Constructed from Non-relational Data) 节点(数据)之间没有显式地定义出关系(非显示图) relation 需要特定规则或者计算方式表达出来 e.g., 比如计算机 3D视觉中的点云数据，如果我们将节点之间的空间距离转化成关系的话，点云数据就成 了图数据。 1.4 图数据的相关任务 节点层面任务(Node Level) 主要包括分类和回归两种任务-&gt;对节点的性质进行预测 需要考虑到节点和节点之间的关系 边层面任务(Link Level) 主要包括边的分类和边的预测任务-&gt;边分类是对边的性质进行预测; 边预测是对两个节点之间是否构成边进行预测 边预测任务主要集中在推荐业务 图层面任务(Graph Level) 主要针对整体结构分析 图层面的任务主要应用在自然科学研究领域，比如对药物分子的分类、酶的分类等","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://xu-shuhao.github.io/categories/DeepLearning/"}],"tags":[{"name":"Learning Note","slug":"Learning-Note","permalink":"https://xu-shuhao.github.io/tags/Learning-Note/"},{"name":"Graph","slug":"Graph","permalink":"https://xu-shuhao.github.io/tags/Graph/"}]},{"title":"Graph Structured Network for Image-Text Matching 阅读笔记","slug":"Graph Structured Network for Image-Text Matching 阅读笔记","date":"2021-08-07T06:16:00.000Z","updated":"2021-08-08T08:41:12.748Z","comments":true,"path":"2021/08/07/Graph Structured Network for Image-Text Matching 阅读笔记/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/07/Graph%20Structured%20Network%20for%20Image-Text%20Matching%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Graph Structured Network for Image-Text Matching 阅读笔记将经过自己理解的知识, 进行处理; PPT 作者介绍 1 在这篇论文出来前, 行业内的难点是什么​ 这是针对imge-text matching的任务, 这个任务的难点有3个: 如何从text文本中提取有效信息, 并且这些信息和信息之间有什么关联 如何从img中获取图片中有物体(目标)的区域, 这些区域和区域之间是有什么关联 最重要的: 如何将text的feature和img的feature产生关联性 2 行业往往用什么办法来解决的​ 行业内常见的做法是将训练数据中的{image, text}这样的pair进行高维度空间的映射, 尽可能地使得image和text在高维度间的距离相近. 这是对象共现的做法, 目前主要有两种学习方案: global correspondence: 学习整个图像与句子之间的对应关系 主要目标是最大化匹配图文对的相似性.该领域的主要研究思路是首先将图像和文本表示为特征向量，然后将它们投影到一个经过排序损失优化的公共空间中.这种全局对应学习方法不能准确地学习图像和文本的对应关系，因为主要对象在图像-文本对的全局表示中占据主导地位，而次要对象大多被忽略. local correspondence: 在局部区域和文字之间学习 缺点是: 忽略了其他的文字对匹配对象的辅助作用 作者提出了什么新思路来解决难点的​ 作者认为行业内的做法没有考虑到被称作fine-grained correspondence的数据, 比如物体和文本的关系, 属性等特征, 只考虑到物体对应. 这样做的后果有两个: 研究方向聚焦物体对应方面, 容易忽略其他有用的信息, 比如说: 关系, 属性等等. 文本对象, 图像对象有可能存在匹配失误的问题, 比如说: 狗 GSMN的结构详细介绍 它由三个模块组成:(a)特征提取：使用Faster-RCNN 和Stanford CoreNLP 分别检测显著区域并解析语义依赖性 (b) 图的构造：图的节点是对象，关系或属性，如果任意两个节点在语义上是相关的，则边存在。(c1)节点级匹配：分别学习对象，关系和属性的对应关系。(c2)结构级匹配：将学习到的对应关系传播给邻居，以共同推断出细粒度的短语对应关系. 1 图结构介绍结构是什么, 是怎么存储的. Textual GraphTextual Graph用无向图 $G_1=(V_1, E_1)$ 表示. 该图也被设置为全连接图 建立这张图, 用到了3个矩阵: 带有self-loops(对角线值为 $1$ )的邻接矩阵 $A$ , 表示单词之间相近度的矩阵 $S$ , 边的权重矩阵 $W_e$ 邻接矩阵$A$是通过Stanford CoreNLP对文本单词进行关系提取, 有语意关系的认为是有边的 $S$矩阵是通过学习得到的, 学习公式是: ${s_{ij} = \\frac{\\exp({\\lambda} {u_i^T} {u_j})} {\\sum_{j=0}^{m} \\exp({\\lambda} {u_i^T} {u_j})} }$ 其中, $u$是word representations(Embeddings),$s_{ij}$ 代表$i$-th和$j$-th节点的相似度 $W$矩阵是通过$S$和$A$得到的, 公式是: $W_e = \\left|S \\circ A \\right|_2$ Visual Graph​ $G_2=(V_2, E_2)$, 同理, 只是将Stanford CoreNLP替换成了Faster-RCNN, 这里作者说了极坐标表示, 我还不是很懂.. 2 图的交互和传播​ 本次的目标是要创造一个函数g( ), 表示$G_1$和$G_2$的相似度, $G_1$节点用$U_{\\alpha}\\in \\Reals^{m \\times d}$表示, m是节点数, d是节点embedding的Dim; $G_2$节点用$V_{\\beta}\\in \\Reals^{n \\times d}$表示, m是节点数, d是节点embedding的Dim. Node-level matching​ 就是用节点和节点之间内积再经过softmax处理之后 表示两个节点之间的近似度; 具体公式是: $C_{t \\to i} = softmax_{\\beta}(\\lambda U_{\\alpha}V_{\\beta}^T)V_\\beta$ ​ $\\lambda$是个超参数, 具体解释请看论文, 这个不是很重要的地方,有趣的地方来了 看到这我一直在思考, 如果按照作者这种建模方式, 岂不是每一对&#123;img, text&#125;的pair都需要单独建模, 建图, 一方面我认为这种方式建模复杂, 不具有很好的范性; 另一方面, 这种text和img的匹配空间就是训练集给什么pair, 测试的时候只能出现相应的pair, 这显然是不合理的. 作者做了如下改变: ​ 将文本图的第i个节点分成t份, 其有关系的视觉图的k个节点也用t份block表示. (具体怎么表示的, 请看作者代码), 最后针对这个第i个节点, 形成了$x_i = x_{i1} |…|x_{it}$, 我的理解是这个$x_i$表示该文本图的节点所有可能匹配的视觉图的节点的匹配关系. ​ 视觉的点的建模方式和文本相似 $C_{i \\to t} = softmax_{\\alpha}(\\lambda V_{\\beta} U_{\\alpha}^T)U_{\\alpha}$ ​ 因为最后形成的$x_i$是针对每一个node以及其所匹配的另一张图的节点的, 所以$C_{t \\to i} \\neq C_{i \\to t}$ Structure-level matching​ 此小节将$x_i$(节点匹配结果)作为输入, 在图中传播, 注意这个structure传播是单独在Text 或者Graph的图上传播, 不涉及同时在两张图传播. 图卷积传播公式是 $\\hat{x_i} = |{k=1}^{K} \\sigma(\\displaystyle\\sum{j \\in N_i} W_e W_k x_j +b)$ 其中K是超参数, 代码中设置为8, $W_k$&amp;$b$是从$K$中学到的参数; 这是每个节点之间的Embedding传播公式 ​ 当节点传播完毕之后, 要计算$Text \\to Image$ &amp;$Image \\to Text$ 分别从Text图, Image图去计算和对方的匹配程度. 所以需要计算$Text \\to Image$图到$Image \\to Text$图的相似度 $s_{t \\rightarrow i}=\\frac{1}{n} \\sum_{i} W_{s}^{u}\\left(\\sigma\\left(W_{h}^{u} \\hat{x}{i}+b{h}^{u}\\right)\\right)+b_{s}^{u}$ $s_{i \\rightarrow t}=\\frac{1}{m} \\sum_{j} W_{s}^{u}\\left(\\sigma\\left(W_{h}^{u} \\hat{x}{i}+b{h}^{u}\\right)\\right)+b_{s}^{u}$ 最后计算两张图的相似度作者是通过+来实现的 $g(G_1, G_2) = s_{t \\rightarrow i} + s_{i \\rightarrow t}$ Loss 损失函数 作者这里设置了 positive sample 和 negative sample 的pair对, 所以损失函数是 $L=\\sum_{(I, T)}\\left[\\gamma-g(I, T)+g\\left(I, T^{\\prime}\\right)\\right]{+}+\\left[\\gamma-g(I, T)+g\\left(I^{\\prime}, T\\right)\\right]{+}$ where$ I^{\\prime} , T^{\\prime}$ are hard negatives, the function$ [·]$ + is equivalent to $max[·, 0]$ 实验部分阅读总结​ 作者文章逻辑很清晰, 这是值得学习的; Q1、是不是一张图片，一段文字就是一张图？ ​ ?? 2、图片矩阵m和n是什么意思 3、为什么区分 i-&gt;t t-&gt;i两种呢？ ​ 因为从两个角度出发, 以i为中心, 记录与它最匹配的相似的图; 所以不一定是相互的. ​ ??","categories":[{"name":"CV","slug":"CV","permalink":"https://xu-shuhao.github.io/categories/CV/"}],"tags":[{"name":"GNN","slug":"GNN","permalink":"https://xu-shuhao.github.io/tags/GNN/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://xu-shuhao.github.io/tags/DeepLearning/"},{"name":"Paper","slug":"Paper","permalink":"https://xu-shuhao.github.io/tags/Paper/"},{"name":"CV","slug":"CV","permalink":"https://xu-shuhao.github.io/tags/CV/"}]},{"title":"新手PytorchTensor笔记 (一)","slug":"新手PytorchTensor笔记","date":"2021-08-06T08:04:20.000Z","updated":"2021-08-27T08:33:21.891Z","comments":true,"path":"2021/08/06/新手PytorchTensor笔记/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/06/%E6%96%B0%E6%89%8BPytorchTensor%E7%AC%94%E8%AE%B0/","excerpt":"","text":"本篇博客介绍Pytorch中的Tensor概念, 如果你会用Numpy的话, 学习轻而易举 来源官方文档: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label 12import torchimport numpy as np Tensor Initialization 张量初始化总结: 从普通的数据转变, 直接torch.tensor() 从Numpy导入, torch.from_numpy() 抄其他数据格式, torch.ones/rand_like() ① 从数据中生成Tensor张量 12data = [1, 2, 3, 4]t_data = torch.tensor(data) ② 从Numpy中导入 12np_array = np.array(data)t_np_array = torch.from_numpy(np_array) ③ 从其他Tensor导入 12t_ones = torch.ones_like(t_np_array)t_rand = torch.rand_like(t_ones) One Tensor: ​ tensor([1, 1, 1, 1]) ​ tensor([0.2233, 0.5553, 0.3333, 0.5542]) ④ 随机生成*(需指定样式shape)* 1234shape = (2, 3, )rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape) 1234567# 额外讲讲 shape 的事情x = torch.rand((2, 3, )) # x的shape值是(2, 3, ), 表示这是一个二维数组, 有两行, 三列y = torch.rand((2, ))# y的shape是(2, ), 表示的是一个一维数组, 这个一维数组里, 有两个元素z = torch.rand((2, 1 ))# z的shape是(2, 1), 表示的是一个二维数组, 总共两行, 一列(每一行一个元素) Tensor Attributes 张量属性 tensor.shape tensor.dtype tensor.devices result is cpu or gpu Tensor Operation123# We move our tensor to the GPU if availableif torch.cuda.is_available(): tensor = tensor.to(&#x27;cuda&#x27;) torch的用法和Numpy很相近 12tensor = torch.ones(4, 4)tensor[:,1] = 0 tensor([ [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) 12t1 = torch.cat([tensor, tensor, tensor], dim=1)# 这里cat 将多个tensor按照 *列* 进行组合, 前提是除了 *列* 之外, 其他维度都要一样 tensor([ [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) 用_下缀会是全体的结果 1tensor.add_(5) #是tensor中每一个单元都+5 Torch的矩阵乘法两种形式: * (torch.mul()) torch.mm() 第一种: * (torch.mul())element-wise乘法, 支持broadcast 怎么记: 方便 矩阵之间元素对应位置乘是不需要动脑子, 很方便算出来的 *号也是很方便打的 乘法的英文是multiply, 为了方便, 缩写成 mul Broadcast 常数(标量)$k$ 内部元素每个与常数$k$ 相乘 行向量 $b$-&gt;必须a的最后一个维度和b的最后一个维度相匹配才行 eg: 12345a = torch.ones(3, 4)b = torch.Tensor([1, 2, 3, 4])tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) 12345a = torch.ones(3, 4)b = torch.Tensor([1, 2, 3])print(a*b)RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1 列向量-&gt;那就是两者的第一个维度要相同咯 结果就是按列用*相乘 矩阵的broadcast 本质上是允许低维度矩阵$A$和高维度矩阵$B$的乘法 实质上的行动是将高维度$B$理解成一群由以$A$的size()为基本单位$C$的矩阵组成, 每个$C$和$A$相乘element-wise 12345678910111213 a = torch.tensor([[1, 2], [2, 3]]) b = torch.tensor([[[1, 2], [2, 3]], [[-1, -2], [-2, -3]], [[1,1], [1, 1]]]) torch.Size([2, 2])torch.Size([3, 2, 2])tensor([[[ 1, 4], [ 4, 9]], [[-1, -4], [-4, -9]], [[ 1, 2], [ 2, 3]]]) 第二种 torch.mm()不支持broadcast-&gt;torch.matmul()支持 怎么记: matrix multiply的缩写(double m) 强调: 这是矩阵的乘法, 矩阵的定义是: 二维,不是一维也不是多维 第三种 torch.matmul()支持broadcast版的torch.mm() 怎么记: 升级版的mm, 字数字数要多一点了 mm-&gt;mat(trix)mul(tiply) 原理和*一样(将大矩阵分成许多由同小矩阵相同size的矩阵组成, 每个小矩阵和小矩阵进行矩阵相乘), 强调一下, 对应位置的矩阵维度要满足矩阵相乘的条件 eg: 1234567891011a = torch.ones(3,4)b = torch.ones(5,2,4)print(torch.matmul(a, b).size())RuntimeError: mat1 and mat2 shapes cannot be multiplied (20x2 and 4x3) a = torch.ones(3,4) b = torch.ones(5,4,2) print(torch.matmul(a, b).size()) torch.Size([5, 3, 2]) #本质是5个(4, 2)矩阵和 a矩阵相乘","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/categories/Pytorch/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/tags/Pytorch/"},{"name":"Learning Note","slug":"Learning-Note","permalink":"https://xu-shuhao.github.io/tags/Learning-Note/"},{"name":"Muggle","slug":"Muggle","permalink":"https://xu-shuhao.github.io/tags/Muggle/"}]},{"title":"Hexo+Github+Butterfly mac上部署自己的博客","slug":"Hexo-Github-Butterfly-mac上部署自己的博客","date":"2021-08-06T03:17:57.000Z","updated":"2021-08-06T05:50:46.542Z","comments":true,"path":"2021/08/06/Hexo-Github-Butterfly-mac上部署自己的博客/","link":"","permalink":"https://xu-shuhao.github.io/2021/08/06/Hexo-Github-Butterfly-mac%E4%B8%8A%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"Hexo+Github+Butterfly mac上部署自己的博客Abstract：摘要 本篇博客在继承诸多博主的经验以及总结官方教程之后，详细讲述如何在MacOS环境下利用Hexo框架下的Butterfly主题，在同性交友网站Github上部署自己的博客。 Introduction：在Github上利用Hexo部署自己的博客已经是个常态，（默认看本教程的用户有Github账号），本文会讲述以下内容： 如何在本地电脑上配置Github公钥和私钥 如何创建Github静态托管页面 如何部署Hexo在本地 如何应用Butterfly主题 如何写博客并且发布到Github上 如何在本地电脑上配置Github公钥和私钥现在打开电脑的terminal终端 1、安装Git​ 如果不确定是否安装过Git，请用 1git --version ​ 如果出现`git version 2.31.1Git版本字样，说明安装成功Git了，如果没有安装过，请用 1brew install git ​ 这是用HomeBrew安装Git包，本博客推荐（Homebrew是MacOS的一款包管理器，类似于python中的pip），如果没有安装HomeBrew，请用 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; 2、生成SSH密钥文件​ 这是为了以后你每次将博客推送到Github，不需要重复输入用户名和密码。 a. 检查是否已经有SSH Key了12cd ~/.sshls ​ 如果文件存在，那就不需要配置SSH密钥，直接跳过这一步骤，如果没有，则在命令行输入： 12git config --global user.name &quot;你的Github用户名&quot;git config --global user.email &quot;你的Github注册邮箱&quot; ​ 随后生成SSH密钥文件 1ssh-keygen -t rsa -C &quot;你的Github注册邮箱&quot; ​ 系统会问你三次密码：比如推送Git时要需要的代码等，个人建议直接三次回车，省得麻烦。 ​ 随后系统会告诉你已经成功生成了id_rsa&amp;id_rsa.pub文件，输入 1cat ~/.ssh/id_rsa.pub ​ 将id_rsa.pub在terminal显示，复制该内容SHA256:xxx到剪切板，随后打开Github，在Setting-&gt;SSH and GPG keys点击New SSH key将剪切板的内容复制到Key上去，Title部分取名容易知道这是哪台电脑的SSH即可. 比如 SimonDeMac 3 验证是否成功输入 1ssh -T git@github.com ​ 进行验证,如果出现Hi 你的Github用户名! You&#39;ve successfully authenticated, but GitHub does not provide shell access.说明验成功了. 如何创建Github静态托管页面​ 新建一个Repositoriy, 名字是你的用户名.github.io, 仓库要选择Public. 至于协议之类的, 都随意. 如何部署Hexo在电脑本地1 查验是否安装Node.js​ Hexo是基于Node.js框架的, 输入node --v如果出现版本号, 就是已经安装过了, 如果没有, 请 1brew install node 2 用npm 安装Hexo​ npm是node,js的包管理工具, 输入npm -v查看是否安装成功, 随后用npm安装Hexo 1npm install -g hexo-cli 3 初始化博客目录​ 比如说你想在/Users/用户名/Documents/GithubBlog这个目录下建立博客, 先将命令行切过去 1cd /Users/你的用户名/Documents ​ 然后 1mkdir GithubBlog ​ 创建目录, 再cd GithubBlog进入,这时候你的目录是/Users/用户名/Documents/GithubBlog ​ 下一步, 初始化博客 1hexo init 4 如何应用Butterfly主题a 在刚才文件夹目录下, 输入1git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/Butterfly ​ 启用Butterfly, vim 根目录的_config,yml文件, 将theme: landscape 改成 theme: Butterfly ​ 为了主题的平滑升级, 把主题默认配置文件themes/Butterfly/_config.yml复制到 Hexo 工作目录下的themes/source/_data/butterfly.yml，如果source/_data的目录不存在那就创建一个。 ​ 如果创建了butterfly.yml, 它将会替换主题默认配置文件themes/Butterfly/_config.yml里的配置项 (不是合并而是替换), 之后就只需要通过git pull的方式平滑地升级 theme-butterfly了。 b 解决Bug出现时 extends includes/layout.pug block content #recent-posts.recent-posts include includes/recent-posts.pug include includes/pagination.pug #aside_content.aside_content include includes/aside.pug 请使用 1npm install hexo-renderer-pug hexo-renderer-stylus --save c 进行_config.yml配置​ 在_config.yml中更新deploy内容: 123456# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: git repo: https://github.com/Github用户名/Github用户名.github.io.git branch: master ​ 因为你选择的是master分支, 所以去Github, 你的用户名.github.io这个仓库中, 打开Setting-&gt; Pages-&gt;Source Branch: 从main改成master 5 如何写博客并且发布到Github上​ 你的博客都在sourc/_post中, hexo -n blogName.md, 用Typora开始写作吧, 写完之后, 在命令行输入 123hexo cleanhexo ghexo d ​ 即可. 关于美化部分, 打算参考这篇文章 [美化Butterfly][https://innerspace-hs.github.io/2020/11/07/butterfly%E7%BE%8E%E5%8C%96hexo%E5%8D%9A%E5%AE%A2/] [][]","categories":[{"name":"Blog","slug":"Blog","permalink":"https://xu-shuhao.github.io/categories/Blog/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://xu-shuhao.github.io/tags/Hexo/"},{"name":"Butterfly","slug":"Butterfly","permalink":"https://xu-shuhao.github.io/tags/Butterfly/"}]}],"categories":[{"name":"Coding","slug":"Coding","permalink":"https://xu-shuhao.github.io/categories/Coding/"},{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/categories/Pytorch/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://xu-shuhao.github.io/categories/DeepLearning/"},{"name":"CV","slug":"CV","permalink":"https://xu-shuhao.github.io/categories/CV/"},{"name":"Blog","slug":"Blog","permalink":"https://xu-shuhao.github.io/categories/Blog/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://xu-shuhao.github.io/tags/Leetcode/"},{"name":"Pytorch","slug":"Pytorch","permalink":"https://xu-shuhao.github.io/tags/Pytorch/"},{"name":"Learning Note","slug":"Learning-Note","permalink":"https://xu-shuhao.github.io/tags/Learning-Note/"},{"name":"Graph","slug":"Graph","permalink":"https://xu-shuhao.github.io/tags/Graph/"},{"name":"GNN","slug":"GNN","permalink":"https://xu-shuhao.github.io/tags/GNN/"},{"name":"DeepLearning","slug":"DeepLearning","permalink":"https://xu-shuhao.github.io/tags/DeepLearning/"},{"name":"Paper","slug":"Paper","permalink":"https://xu-shuhao.github.io/tags/Paper/"},{"name":"CV","slug":"CV","permalink":"https://xu-shuhao.github.io/tags/CV/"},{"name":"Muggle","slug":"Muggle","permalink":"https://xu-shuhao.github.io/tags/Muggle/"},{"name":"Hexo","slug":"Hexo","permalink":"https://xu-shuhao.github.io/tags/Hexo/"},{"name":"Butterfly","slug":"Butterfly","permalink":"https://xu-shuhao.github.io/tags/Butterfly/"}]}